server:
  port: 8080

spring:
  application:
    name: "llm-mcp-demo"
  cache:
    type: caffeine

openweather:
  api:
    key: ${OPENWEATHER_API_KEY:"DUMMY"}
    base-url: https://api.openweathermap.org/data/2.5

openai:
  api:
    key: ${OPENAI_API_KEY:"DUMMY"}
    chat-url: https://api.openai.com/v1/chat/completions
    model: gpt-4o-mini

resilience4j:
  retry:
    instances:
      openweather:
        max-attempts: 3
        wait-duration: 500ms
  circuitbreaker:
    instances:
      openweather:
        sliding-window-size: 10
        failure-rate-threshold: 50
        minimum-number-of-calls: 5
